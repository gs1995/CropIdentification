{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "#from osgeo import gdal\n",
    "from skimage import exposure\n",
    "from skimage.segmentation import quickshift, felzenszwalb\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RASTER_DATA_FILE = \"data/image/2298119ene2016recorteTT.tif\"\n",
    "TRAIN_DATA_PATH = \"data/train/\"\n",
    "TEST_DATA_PATH = \"data/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 1000)\n"
     ]
    }
   ],
   "source": [
    "from pylab import *\n",
    "verification_pixels=load(\"test.npy\")\n",
    "print(verification_pixels.shape)\n",
    "for_verification = np.nonzero(verification_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5414f9555b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mverification_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverification_pixels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfor_verification\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfor_verification\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "verification_labels = verification_pixels[for_verification]\n",
    "predicted_labels = clf[for_verification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir(TRAIN_DATA_PATH) if f.endswith('.shp')]\n",
    "classes_labels = [f.split('.')[0] for f in files]\n",
    "#print(classes_labels)\n",
    "segments=load(\"segments.npy\")\n",
    "clf = np.copy(segments)\n",
    "for_verification = np.nonzero(verification_pixels)\n",
    "verification_labels = verification_pixels[for_verification]\n",
    "predicted_labels = clf[for_verification]\n",
    "cm = metrics.confusion_matrix(verification_labels, predicted_labels)\n",
    "def print_cm(cm, labels):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    # https://gist.github.com/ClementC/acf8d5f21fd91c674808\n",
    "    columnwidth = max([len(x) for x in labels])\n",
    "    # Print header\n",
    "    print(\" \" * columnwidth, end=\"\\t\")\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=\"\\t\")\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"%{0}s\".format(columnwidth) % label1, end=\"\\t\")\n",
    "        for j in range(len(labels)):\n",
    "            print(\"%{0}d\".format(columnwidth) % cm[i, j], end=\"\\t\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \tD\tB\tA\tE\tC\t\n",
      "D\t0\t0\t0\t0\t0\t\n",
      "B\t0\t0\t0\t0\t0\t\n",
      "A\t0\t0\t0\t0\t0\t\n",
      "E\t0\t0\t0\t0\t0\t\n",
      "C\t0\t0\t0\t0\t0\t\n"
     ]
    }
   ],
   "source": [
    "print_cm(cm, classes_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification accuracy: %f\" %\n",
    "\n",
    "      metrics.accuracy_score(verification_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          D       0.00      0.00      0.00      1905\n",
      "          B       0.00      0.00      0.00        65\n",
      "          A       0.00      0.00      0.00        88\n",
      "          E       0.00      0.00      0.00       180\n",
      "          C       0.00      0.00      0.00       160\n",
      "\n",
      "avg / total       0.00      0.00      0.00      2398\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glob/intel-python/versions/2018u1/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1428: UserWarning: labels size, 168, does not match size of target_names, 5\n",
      "  .format(len(labels), len(target_names))\n",
      "/glob/intel-python/versions/2018u1/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/glob/intel-python/versions/2018u1/intelpython3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report:\\n%s\" %\n",
    "      metrics.classification_report(verification_labels, predicted_labels,\n",
    "                                    target_names=classes_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 1)",
   "language": "python",
   "name": "intel_distribution_of_python_3_2018u1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
